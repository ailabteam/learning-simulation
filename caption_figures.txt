Figure 1: Performance of Routing Strategies Over Time.
This figure illustrates the end-to-end one-way delay in milliseconds (ms) over a sequence of 50 time slots for a connection between Hanoi and Rio de Janeiro. The blue solid line represents the Classic Dijkstra algorithm, which always selects the path with the absolute lowest delay at each instant. The orange dashed line represents our proposed AI-Powered routing strategy, which selects a path based on the AI model's prediction of link-state delays. The key observation is that while the Classic Dijkstra route consistently achieves a lower delay, the AI-Powered route's delay fluctuates more significantly.
Figure 2: Comparison of End-to-End Delay Distribution (Jitter).
This box plot compares the statistical distribution of one-way delays for the Classic Dijkstra and AI-Powered routing strategies over the entire simulation period. Each box represents the interquartile range (IQR), with the central line indicating the median delay. The whiskers extend to 1.5 times the IQR. The results clearly show that the AI-Powered strategy not only has a higher median delay but also a significantly larger IQR and wider range, indicating a higher delay jitter (0.001846 std dev) compared to the more stable Classic Dijkstra approach (0.000511 std dev). This outcome did not support our initial hypothesis and suggests that the features used for the AI model were insufficient to capture the dynamics required for path stabilization.
Figure 3: Network Recovery Time After Critical Satellite Failure.
This figure compares the network recovery time between a reactive and our proposed proactive (AI-assisted) strategy following the simulated failure of a critical satellite on the Beijing-New York route. The reactive strategy, which recalculates the shortest path only after detecting the failure, exhibits a measurable recovery downtime of 0.54 ms. In contrast, our proactive strategy, which pre-computes a backup path by identifying high-risk nodes via Betweenness Centrality, achieves a near-instantaneous recovery time (~0 ms). This demonstrates the significant potential of proactive resilience schemes in minimizing service disruption in mission-critical LEO network applications.
Figure 4: Performance Comparison of Beam Steering Policies.
This figure presents the total reward, defined as the sum of covered user demand, achieved by three different satellite beam steering policies over a full orbital period. The "Random" policy serves as a lower-bound baseline. The "Greedy" policy, which has perfect knowledge of the user demand map, represents a near-optimal upper bound. Our trained Deep Reinforcement Learning (DRL) agent significantly outperforms the Random policy, achieving nearly 6 times the total reward. While it currently reaches approximately 45% of the Greedy policy's performance, this result strongly validates the DRL approach, demonstrating that an agent with limited state information (only its own coordinates) can successfully learn a highly effective resource allocation strategy.

